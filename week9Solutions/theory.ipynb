{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 9 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1:  Maximal Margin By Hand \n",
    "\n",
    "- Assume you are given two points $x_1=(1, 1)$ with class -1 and $x_2 = (11, 11)$ with class 1. What is the maximum separating hyperplane. E.g. what are vectors w and b that maximally separates these points? What are the support vectors? What is the margin? Try visualizing the data set.\n",
    "- If we have three points in class -1 (-10, -10), (-5, 2), (1, 1) and four points in class 1 (20, 23), (15, 17), (12, 10), (10, 12). What is the maximal separating hyperplane (w,b)? Try visualizing the data set before you work too hard.\n",
    "What are the support vectors? What is the margin? You can run the code below the next exercise to get the answers from actually running the python sklearn SVM implementation on the data.\n",
    "- Write down the exact form of the SVM problem we need to solve, if the input data is defined as the 7 points above.\n",
    "The convex quadratic program was defined as \n",
    "\n",
    "$\\min_{w, b} \\frac{1}{2}{||w||^2}$\n",
    "\n",
    "s.t. $\\forall i : y_i(w^\\intercal x_i + b) \\geq 1$\n",
    "\n",
    "You job is thus to write down the constraints.\n",
    "\n",
    "\n",
    "### SOLUTION MATH\n",
    "1. The maximum separating hyperplane is the bisector between the two points. Its normal vector is $w = (1/\\sqrt{2},1/\\sqrt{2})$. With that normal vector, the margins become $(11,11)^\\intercal (1/\\sqrt{2}, 1/\\sqrt{2}) + b = b + \\sqrt{2} \\cdot 11$ and $(-1)((1,1)^\\intercal(1/\\sqrt{2},1/\\sqrt{2}) + b) = -b - \\sqrt{2}$. Choosing $b$ such that $b + \\sqrt{2} \\cdot 11 = -b - \\sqrt{2}$ gives $b = -12/\\sqrt{2}$. The margin is then $-12/\\sqrt{2} + \\sqrt{2} \\cdot 11 = -12/\\sqrt{2} + 22 /\\sqrt{2} = 10/\\sqrt{2}$.\n",
    "\n",
    "SVM-problem: Notice that $w$ is on the form $(w_1,w_1)$ - since it has the direction of the bisector between $x_1$ and $x_2$. Further notice that if we have a solution $w,b$ such that $y_1(w^Tx_1+b)=-(w^Tx_1+b)>1$ or $y_2(w^Tx_2+b)=(w^Tx_2+b)>1$ we may change $b$ such that both constraints holds with strict inequalities, and then decrease the length of $w$ until one of the constraints holds with equality, thus the solution $w,b$ could be made to a new solution $w',b'$ where the length of $w'$ is smaller and thus the loss smaller. Hence an optimal solution $w,b$ must be such that  $-(2w_1+b)=-(w^Tx_1+b)=1$ and $2\\cdot 11w_1+b=(w^Tx_2+b)=1$. Now solving this set of linear equations in $w_1$ and $b$ we get that $w_1=1/10$ implying $w=(1/10,1/10)$ and $b=-12/10$ (as the print of sklearns solution in exercise 2 also shows). \n",
    "\n",
    "\n",
    "2. The new points do not change the maximal hyperplane\n",
    "\n",
    "3. The constraints become:\n",
    "\n",
    "$(-1)(-10 w_1 - 10 w_2 + b) \\geq 1$\n",
    "\n",
    "$(-1)(-5 w_1 + 2 w_2 + b) \\geq 1$\n",
    "\n",
    "$(-1)(w_1 + w_2 + b) \\geq 1$\n",
    "\n",
    "$(20 w_1 + 23 w_2 + b) \\geq 1$\n",
    "\n",
    "$(15 w_1 + 17 w_2 + b) \\geq 1$\n",
    "\n",
    "$(12 w_1 + 10 w_2 + b) \\geq 1$\n",
    "\n",
    "$(10 w_1 + 12 w_2 + b) \\geq 1$\n",
    "\n",
    "### end solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: Regularized SVM\n",
    "\n",
    "Consider a regularized SVM with regularization hyperparameter C.\n",
    "\n",
    "$\\min_{w, b, \\xi} \\frac{1}{2}{||w||^2} + C \\sum_{i=1}^n \\xi_i$\n",
    "\n",
    "s.t. $\\forall i : y_i(w^\\intercal x_i + b) \\geq 1 - \\xi_i$\n",
    "\n",
    "and $\\forall i : \\xi_i \\geq 0$\n",
    "\n",
    "Assume you are given three points $x_1=(1, 1)$ with class -1 and $x_2=(3, 3), x_3 = (11, 11)$ with class 1.\n",
    "- Write down the exact form of the SVM problem we need to solve if the input data is $D=\\{x_1, x_2, x_3\\}$.\n",
    "  Thus, your job is thus to write down the constraints.\n",
    "\n",
    "- What is the best cost you can get when using the hyperplane $w = [ 0.1, 0.1]$ and  $b: -1.2$. i.e. how can you pick $\\xi_1, \\xi_2, \\xi_3$ such that the constraints are satisfied while minimizing the cost with w and b  fixed.\n",
    "\n",
    "- Is there a general way given, $w$ and $b$, to compute the best $\\xi_i$ \n",
    "- Can you find a $w, b$ with a smaller cost for $C=1$? If you like, you can use the svm code below to experiment.\n",
    "  \n",
    "\n",
    "\n",
    "Also be sure to see how to apply an SVM implementation on data using Sklearn.\n",
    "\n",
    "### solution math\n",
    "1.\n",
    "  \n",
    "  $(-1)(w_1 + w_2 + b) \\geq 1-\\xi_1$\n",
    "  \n",
    "  $(3w_1 + 3w_2 + b) \\geq 1-\\xi_2$\n",
    "  \n",
    "  $(11w_1 + 11w_2 + b) \\geq 1-\\xi_3$\n",
    "  \n",
    "2.\n",
    "The constraints become:\n",
    "\n",
    "$(-1)(0.1 + 0.1 - 1.2) \\geq 1-\\xi_1 \\Rightarrow \\xi_1 \\geq 0$\n",
    "\n",
    "$(0.3 + 0.3 - 1.2) \\geq 1-\\xi_2 \\Rightarrow \\xi_2 \\geq 1.6$\n",
    "\n",
    "$(1.1 + 1.1 - 1.2) \\geq 1-\\xi_3 \\Rightarrow \\xi_3 \\geq 0$\n",
    "\n",
    "Hence the best is to choose $\\xi_1=\\xi_3 = 0$ and $\\xi_2 = 1.6$, giving a cost of $\\frac{1}{2}{||w||^2} + 1.6 C = 0.01 + 1.6 C$.\n",
    "\n",
    "3.\n",
    "Yes, simply solve $y_i(w^\\intercal x_i + b) = 1 - \\xi_i$. The best choice is thus $\\xi_i = \\max(0,1-y_i(w^\\intercal x_i + b))$.\n",
    "\n",
    "4.\n",
    " We can use the separating hyperplane $w=(1/\\sqrt{2}, 1/\\sqrt{2})$ with $b=-2\\sqrt{2}$. Then all constraints are satisfied with $\\xi_i = 0$ since:\n",
    "  \n",
    "  $(-1)(1/\\sqrt{2}+1/\\sqrt{2} - 2\\sqrt{2}) \\geq 1 \\Leftrightarrow \\sqrt{2} \\geq 1$\n",
    "  \n",
    "  $(3/\\sqrt{2}+3/\\sqrt{2} - 2\\sqrt{2}) \\geq 1 \\Leftrightarrow \\sqrt{2} \\geq 1$\n",
    "  \n",
    "  $(11/\\sqrt{2}+11/\\sqrt{2} - 2\\sqrt{2}) \\geq 1 \\Leftrightarrow 9 \\sqrt{2} \\geq 1$\n",
    "      \n",
    "these are clearly all satisfied. The cost then becomes $\\|w\\|^2 + 0 = 1$.\n",
    "### end solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for running the example above\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "def plot_hyperplane(w, *args, **kwargs): \n",
    "    if w[1]==0 and w[2]==0:\n",
    "        raise ValueError('Invalid hyperplane')\n",
    "    xmin, xmax, ymin, ymax = plt.axis()\n",
    "    \n",
    "    if w[2]==0:\n",
    "        # Vertical line\n",
    "        x = np.array((1/w[1], 1/w[1]))\n",
    "        y = np.array((ymin, ymax))\n",
    "    else:\n",
    "        x = np.array((xmin, xmax))\n",
    "        y = (-w[0]-w[1]*x)/w[2]       \n",
    "    # plot the line\n",
    "    plt.plot(x, y, *args, **kwargs)\n",
    "\n",
    "def run_svm(X, Y, kernel='linear', **kwargs):\n",
    "    # fit the model\n",
    "    clf = svm.SVC(kernel=kernel, **kwargs)\n",
    "    clf.fit(X, Y)\n",
    "\n",
    "    # get the separating hyperplane\n",
    "    \n",
    "    print('Hyperplane found w: {0} b: {1}'.format(clf.coef_[0],clf.intercept_[0]))\n",
    "    margin = 1.0/np.linalg.norm(clf.coef_[0])\n",
    "    print('Margin 1/||w||: {0}'.format(margin))\n",
    "    hyp = np.r_[clf.intercept_,clf.coef_[0]]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y,cmap=plt.cm.Paired)\n",
    "    print('Support Vectors:')    \n",
    "    print(clf.support_vectors_)\n",
    "    \n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],s=80, facecolors='none')\n",
    "    plt.xlim(X.min()-1,X.max()+1)\n",
    "    plt.ylim(X.min()-1,X.max()+1)\n",
    "    plot_hyperplane(hyp,'k-')\n",
    "    \n",
    "    print('what is hyp',hyp)\n",
    "    tmp = hyp[0]\n",
    "    hyp[0] = tmp + 1\n",
    "    plot_hyperplane(hyp,'k--')\n",
    "    hyp[0] = tmp -1\n",
    "    plot_hyperplane(hyp,'k--')\n",
    "    hyp[0] = tmp\n",
    "    \n",
    "    #plt.axis('tight')\n",
    "    plt.show()\n",
    "\n",
    "X = np.array([[1, 1], [11, 11]])\n",
    "Y = np.array([0, 1])\n",
    "print('First Exercise From Above')\n",
    "run_svm(X, Y, C=1)\n",
    "plt.show()\n",
    "\n",
    "print('Second Exercise From Above')\n",
    "X = np.array([(-10, -10), (-5, 2), (1, 1),  (20, 23), (15, 17), (12, 10), (10, 12)])\n",
    "Y = np.array([0, 0, 0, 1, 1, 1, 1])\n",
    "run_svm(X, Y)\n",
    "plt.show()\n",
    "\n",
    "#print('Larger Separable Data set')\n",
    "#n = 50\n",
    "#X = 2*np.r_[2*np.random.randn(n, 2) - [4, 4], 2*np.random.randn(n, 2) + [5, 5]]\n",
    "#Y = [-1] * n + [1] * n\n",
    "#run_svm(X, Y)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3: Kernel Perceptron \n",
    "In this exercise you must implement Kernel Perceptron for learning from a stream of data.\n",
    "The learning algorithm will only do one pass over the data like in a system processing an input data stream.\n",
    "\n",
    "The important part is how to actually add Kernels to the Perceptron learning algorithm.\n",
    "\n",
    "First we need to represent the hyperplane in the feature space induced by the Kernel.\n",
    "This must be implemented in the class *Representer* as described below.\n",
    "\n",
    "Remember the Perceptron algorithm, the current solution hyperplane is updated on a mispredicted data point $(x, y)$ as\n",
    "$$\n",
    "w = w + y x\n",
    "$$\n",
    "In this exercise, the hyperplane exists in feature space and must be updated there, but in the same way\n",
    "with one difference. We include a learning rate $\\alpha>0$ that scales the update.\n",
    "The update that must be implemented becomes\n",
    "$$\n",
    "w = w + \\alpha y \\phi(x)\n",
    "$$\n",
    "where $\\phi$ is the feature transform corresponding to the used Kernel. As we will see, we do not really need to know what $\\phi$ is.\n",
    "\n",
    "This means that the hyperplane solution is a linear combination of (transformed) input points and thus can be written as\n",
    "$$\n",
    "w = \\sum_i \\alpha_i \\phi(x_i)\n",
    "$$\n",
    "and may be represented by storing the list of $\\alpha_i$ and $x_i$. Note we have not discussed how to initialize $w$, which can be done just like an update. Also note that we do not use a bias variable in this exercise.\n",
    "\n",
    "\n",
    "**Task:** In the class Representer implement \n",
    "* update(x, $\\alpha$): add point x with weight $\\alpha$ to the hyperplane\n",
    "* dot(z): compute and return \n",
    "$$\n",
    " w^\\intercal \\phi(z) = \\sum_i  \\alpha_i (\\phi(x_i)^\\intercal \\phi(z))  = \\sum_i \\alpha_i K(x_i, z)\n",
    "$$ \n",
    "(note the indexing here is not over the data set but the set of weight and points comprising w - and we assume the lists are non-empty)\n",
    "\n",
    "After you have implemented the representation of the hyperplane you must implement\n",
    "The Perceptron Classifier in the *KernelPerceptron* class.\n",
    "* Implement the score function (compute accuracy of classifier on given data X with labels y)\n",
    "* Implement the fit method - do one scan over the data and for each misprediction (x, y) update $w$ by adding $\\alpha y \\phi(x)$\n",
    "\n",
    "Test your implementation by running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming perceptron with kernels\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data():\n",
    "    \"\"\" Simple helper function for downloading and loading data \"\"\"\n",
    "    filename = 'nonlinear_data.npz'\n",
    "    D = np.load(filename)\n",
    "    return D\n",
    "\n",
    "def visualize_kernel_perceptron(X, Y, w):\n",
    "    \"\"\" Helper function for visualizing decision boundary in input space\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(20, 16))\n",
    "    ax.scatter(X[:, 0], X[:,1 ], c=Y, cmap=plt.cm.Paired, s=20)\n",
    "    nsize = 50\n",
    "    xs = ys = np.linspace(-1, 1, nsize)\n",
    "    xm, ym = np.meshgrid(xs, ys)\n",
    "    img = np.zeros((nsize, nsize)) # makes a 100 x 100 2d array\n",
    "    for i, zy in enumerate(ys):\n",
    "        for j, zx in enumerate(xs):    \n",
    "            point = np.array([zx, zy])\n",
    "            predict = w.dot(point)\n",
    "            img[i, j] = predict\n",
    "    ax.contour(xs, ys, img, [0], colors='r', linewidths=3)\n",
    "    plt.show()\n",
    "\n",
    "def get_rbf_kernel(gamma=1.0):\n",
    "    assert gamma > 0, 'Gamma must be positive'\n",
    "    def K(x, z):\n",
    "        return np.exp((-gamma * np.sum((x-z)**2)))\n",
    "    return K\n",
    "\n",
    "class Representer():\n",
    "    \"\"\" Represents a hyperplane H in Feature space that is a linear combination of transformed points from input space\n",
    "        \n",
    "        The class can evaluate an input in the original input space mapped to H against the hyperplane        \n",
    "    \"\"\"\n",
    "    def __init__(self, K):\n",
    "        self.dat = list()\n",
    "        self.alpha = list()\n",
    "        self.K = K\n",
    "\n",
    "    def update(self, x, a):\n",
    "        \"\"\" Update hyperplane Representer by point x with weight a i.e. w = w + a * x\n",
    "        \n",
    "        Args:\n",
    "            x: np.array - data point\n",
    "            a: float - data weight\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.dat.append(x)\n",
    "        self.alpha.append(a)\n",
    "        ### END CODE\n",
    "\n",
    "    def dot(self, z):\n",
    "        \"\"\" Compute inner product between hyperplane and z in feature (kernel) space\n",
    "                \n",
    "        Args:\n",
    "            z: np.array\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        out =  np.sum([a*self.K(z, x) for (x, a) in zip(self.dat, self.alpha)])\n",
    "        ### END CODE\n",
    "        return out\n",
    "    \n",
    "class KernelPerceptron():\n",
    "    \n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self, X, Y, K, alpha=0.1):\n",
    "        \"\"\" Kernel Perceptron Algorithm \n",
    "            1. Do one pass over the data - for each misprediction (w.dot(x) * y <= 0) add phi(x) with weight y * alpha to hyperplane representation\n",
    "            \n",
    "        \"\"\"\n",
    "        w = Representer(self.K)\n",
    "        ### YOUR CODE HERE\n",
    "        d = X.shape[1]\n",
    "        # start_w = np.zeros(d)\n",
    "        # w.update(start_w, 1)\n",
    "        n = len(Y)\n",
    "        for i in range(n):\n",
    "            x = X[i, :]\n",
    "            y = Y[i]\n",
    "            if w.dot(x) * y <= 0:\n",
    "                w.update(x, y * alpha)                    \n",
    "        self.w = w\n",
    "        ### END CODE\n",
    "        \n",
    "    def score(self, X, Y):\n",
    "        \"\"\" Compute Classifier Accuracy\n",
    "        Args:\n",
    "          X: np.array n, d\n",
    "          Y: np.array n, \n",
    "        \n",
    "        Returns:\n",
    "        out: scalar - accuracy of model on data X with labels Y\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        ### YOUR CODE HERE\n",
    "        pred = np.sign(np.array([self.w.dot(x) for x in X]))\n",
    "        print(pred.shape)\n",
    "        out = (pred == Y).mean()\n",
    "        ### END CODE\n",
    "        return out\n",
    "            \n",
    "    \n",
    "\n",
    "D = load_data()\n",
    "X = D['X4']\n",
    "Y = D['y4']\n",
    "rbf_kernel = get_rbf_kernel(1.0)\n",
    "kernel_perceptron_classifier = KernelPerceptron(rbf_kernel)\n",
    "kernel_perceptron_classifier.fit(X, Y, rbf_kernel)\n",
    "print('In Sample Accuracy after one scan: {0}'.format(kernel_perceptron_classifier.score(X, Y)))\n",
    "visualize_kernel_perceptron(X, Y, kernel_perceptron_classifier.w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4:  Choosing b\n",
    "\n",
    "In the lecture, we saw the dual optimization problem for the regularized version of support vector machines. The optimization problem was as follows:\n",
    "$$\n",
    "\\min_{w,b,\\xi} 1/2 \\|w\\|^2 + C\\sum_{i=1}^n \\xi_i \n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "y_i(w^Tx_i+b) \\geq 1-\\xi_i\n",
    "$$\n",
    "$$\n",
    "\\xi_i \\geq 0\n",
    "$$\n",
    "We also argued that after solving the dual optimization problem, the optimal $w$ satisfies\n",
    "$$\n",
    "w=\\sum_{i=1}^n \\alpha_i y_i x_i\n",
    "$$\n",
    "But finding the optimal $b$ was left as an exercise. We will do that here.\n",
    "\n",
    "So assume we have the optimal $\\alpha$ and hence optimal $w$ and we need to find $b$. Looking at the cost, we see that we need to choose $b$ to minimize $C\\sum_{i=1}^n \\xi_i$ subject to the constraints. The term $1/2 \\|w\\|^2$ is unaffected by the choice of $b$.\n",
    "\n",
    "Next, observe that for any choice of $b$, the optimal choice of $\\xi$ is such that $\\xi_i = \\max\\{0,1-y_i(w^Tx_i+b)\\}$ as this is exactly when the constraints become satisfied and thus contributes the least to the loss. For any choice of $b$, the contribution to the loss thus becomes $C\\sum_{i=1}^n \\max\\{0,1-y_i(w^Tx_i+b)\\}$. Your task is to describe an efficient algorithm for computing the optimal $b$. Hint: You may want to look at the derivative of this loss wrt. $b$ and remember from the lecture that hinge loss is convex.\n",
    "\n",
    "### SOLUTION MATH\n",
    "This is the hinge loss, which is a convex function, so we can minimize it by setting the derivative to $0$. By the chain rule, the derivative is $C \\sum_{i=1}^n -y_i \\cdot 1_{1-y_i(w^Tx_i+b) > 0} = C \\sum_{i=1}^n -y_i \\cdot 1_{y_i(w^Tx_i+b)<1} = C(\\sum_{i : y_i=-1} 1_{b>-1-w^Tx_i}-\\sum_{i : y_i=1} 1_{b < 1-w^Tx_i})$.\n",
    "\n",
    "Examining this expression, we can see that for the terms corresponding to $y_i=1$, the contribution to the derivative is negative when $b$ starts out at $-\\infty$. As $b$ increases, more and more of these terms will disappear from the sum, making the derivative larger and larger. For the terms with $y_i = -1$, the derivative starts out as $0$, but as $b$ increases, more and more of these terms change to $+1$. The changes in derivative happen at the values $-1-w^Tx_i$ and $1-w^Tx_i$ for points with labels $-1$ and $1$ respectively. We can thus compute all these values, sort them, and sweep from smallest to largest while keeping track of the derivative. When it changes from negative to positive, we have found $b$. This takes $O(n \\lg n)$ time for the sweep and $O(nd)$ time to compute $w^Tx_i$ for all $i$. It can also be done with kernels.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Hinge Loss - Cost and Gradient\n",
    "In the lecture, we talked about that we could rewrite the primal problem (with no kernels) into an unconstrained convex minimization problem using hinge loss.\n",
    "\n",
    "For a hypothesis $h$ and a data point $x$ with label $y \\in \\{-1, +1\\}$, the hinge loss is defined as\n",
    "$$\n",
    "e(h(x), y) = \\max(0, 1-h(x)y)\n",
    "$$\n",
    "Define the (average regularized) hinge loss over the data set as\n",
    "$$\n",
    " \\|w\\|^2+\\frac{C}{n} \\sum_{i=1}^n \\max(0, 1-h(x_i)y_i) = \\|w\\|^2+ \\frac{C}{n} \\sum_{i=1}^n \\max(0,1 -(w^\\intercal x_i +b) y_i)\n",
    "$$\n",
    "\n",
    "To make a gradient descent algorithm for Hinge Loss for a linear model from scratch, we would need to compute gradients.\n",
    "\n",
    "**Your Task:** Compute expressions for the gradients $\\nabla_w, \\nabla_b$ of the loss above (average regularized hinge loss over the data set) wrt. $w$ and $b$\n",
    "\n",
    "\n",
    "(we know max(0,x) is not differentiable at zero but let us ignore that and say that the derivative is zero at zero, for more  go to wikipedia and look up subgradient).\n",
    "\n",
    "### SOLUTION MATH\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_w = 2 w - \\frac{C}{n}\\sum_{i=1}^n x_i y_i 1_{1-y_i (w^\\intercal x_i + b)) > 0}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b = -\\frac{C}{n} \\sum_{i=1}^n y_i 1_{1-y_i (w^\\intercal x_i + b)) > 0}\n",
    "$$\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6: Hinge Loss Gradient Descent with Pytorch\n",
    "In this exercise we will see how to use pytorch's built-in optimizer class to do gradient descent for SVM's with Hinge Loss.\n",
    "1. Implement a Hinge Loss Classifier trained by gradient descent in Pytorch by completing the methods in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "n = 100\n",
    "hinge_data_numpy = 2*np.r_[2*np.random.randn(n, 2) - [4., 4.], 2*np.random.randn(n, 2) + [5., 5.]]\n",
    "hinge_labels_numpy = np.array([-1] * n + [1] * n)\n",
    "\n",
    "\n",
    "class HingeLossClassifier():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def hinge_loss(self, X, y, w, b, c):\n",
    "        \"\"\"\n",
    "        Compute c times average Hinge Loss plus ||w||^2 on Torch tensors\n",
    "        torch.sum and x.clamp may com in useful\n",
    "     \n",
    "        Args:\n",
    "            X: torch.tensor shape n,d \n",
    "            y: torch.tensor shape n, \n",
    "            w: torch.tensor shape d,\n",
    "            b: float\n",
    "            c: float scaler for c hinge loss\n",
    "        Returns:\n",
    "        loss torch.tensor 1 x 1     \n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        pred = X @ w + b\n",
    "        loss = (1.0 - pred * y)\n",
    "        loss = loss.clamp(min=0)\n",
    "        # print('loss after', loss)\n",
    "        out = c*torch.mean(loss)\n",
    "        reg = torch.sum(w**2)\n",
    "        out = out + reg\n",
    "        ### END CODE\n",
    "        return out\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Compute predictions on X (predictions in -1,+1)\n",
    "        \n",
    "        Args:\n",
    "            X: torch.tensor shape n,d \n",
    "        \"\"\"\n",
    "        out = None\n",
    "        ### YOUR CODE HERE\n",
    "        out = torch.sign(X@self.w + b)\n",
    "        ### END CODE\n",
    "        return out\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute accuracy of classifier on y\n",
    "        \n",
    "        Args:\n",
    "            X: torch.tensor shape n,d \n",
    "            y: torch.tensor shape n, \n",
    "        \"\"\"\n",
    "        acc = None\n",
    "        ### YOUR CODE HERE\n",
    "        acc = (self.predict(X)==Y).numpy().mean()\n",
    "        ### END CODE\n",
    "        return acc\n",
    "    \n",
    "    def fit(self, X, y, c, epochs, lr):\n",
    "        \"\"\" Gradient Descent Algorithm \n",
    "        \n",
    "        Args:\n",
    "            X: torch.tensor shape n,d \n",
    "            y: torch.tensor shape n, \n",
    "            c: float scalar for c * hinge in loss\n",
    "            epochs: int - number of iterations of gradient descent\n",
    "            lr: float, learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        w = torch.zeros(X.shape[1], requires_grad=True)\n",
    "        b = torch.zeros(1, 1, requires_grad=True)\n",
    "        n = torch.tensor([len(y)],dtype=torch.float64)\n",
    "        optimizer = optim.SGD([w, b], lr=lr)\n",
    "        # remember to call optimizer.zero_grad\n",
    "        ### YOUR CODE HERE\n",
    "        for i in range(epochs):\n",
    "            optimizer.zero_grad() # clears gradients otherwise they are accumulated\n",
    "            loss = self.hinge_loss(X, y, w, b, c)\n",
    "            loss.backward()\n",
    "            acc = torch.sum(((X @ w + b) * y)>0)/n\n",
    "            print('round: {0}, loss: {1}, accuracy: {2}'.format(i, loss.item(), acc.item()))\n",
    "            optimizer.step() # take a gradient step\n",
    "        ### END CODE\n",
    "        self.w = w.detach()\n",
    "        self.b = b.detach()\n",
    "        \n",
    "hinge_data_torch = torch.from_numpy(hinge_data_numpy).float()\n",
    "hinge_labels_torch = torch.from_numpy(hinge_labels_numpy).float()\n",
    "w_test = torch.tensor([-1., -1], requires_grad=True)\n",
    "b_test = torch.tensor([1.0], requires_grad=True)\n",
    "c_test = 1.0\n",
    "\n",
    "print('Check your computations from above')\n",
    "cl = HingeLossClassifier()\n",
    "loss = cl.hinge_loss(hinge_data_torch, hinge_labels_torch, w_test, b_test, c_test)\n",
    "loss.backward()\n",
    "print('check loss and gradient: ', loss.item(),'w grad: ', w_test.grad, 'b grad:', b_test.grad)\n",
    "\n",
    "print('Run Gradient Descent')\n",
    "cl.fit(hinge_data_torch, hinge_labels_torch, 1, 30, 0.1)\n",
    "w = cl.w.numpy()\n",
    "b = cl.b.numpy().ravel()[0]\n",
    "print('Learned w and b', cl.w.numpy(), cl.b.numpy())\n",
    "\n",
    "def plot_hyperplane(ax, w, b, *args, **kwargs): \n",
    "    xmin, xmax, ymin, ymax = ax.axis()\n",
    "    \n",
    "    if w[1]==0:\n",
    "        # Vertical line\n",
    "        print('vert')\n",
    "        x = np.array((1 / w[0], 1 / w[0]))\n",
    "        y = np.array((ymin, ymax))\n",
    "    else:\n",
    "        x = np.array((xmin, xmax))\n",
    "        y = (-b -w[0] * x) / w[1]       \n",
    "    # plot the line\n",
    "    print(x, y)\n",
    "    ax.plot(x, y, *args, **kwargs)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 16))\n",
    "ax.scatter(hinge_data_numpy[:,0], hinge_data_numpy[:, 1], c=hinge_labels_numpy, s=30)\n",
    "margin = 1/np.linalg.norm(w)\n",
    "plot_hyperplane(ax, w, b - 1,'k--', linewidth=2)\n",
    "plot_hyperplane(ax, w, b + 1,'k--', linewidth=2)\n",
    "plot_hyperplane(ax, w, b,'g-', linewidth=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
